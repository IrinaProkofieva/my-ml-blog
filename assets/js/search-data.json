{
  
    
        "post0": {
            "title": "Первый пост",
            "content": "Первый пост . Мой первый пост будет очень коротким, этакая проба пера (и различных возможностей Markdown-а). Когда начинаешь вести блог, сталкиваешься с кучей проблем: . Во-первых, конечно, встает вопрос, есть ли что-то, что ты можешь рассказать окружающим, что еще не было сказано до тебя? Практически всё сейчас ищется в интернете за пару кликов. Да что там! Даже клики делать не надо, просто говоришь: “Ок, Гугл. Могут ли машины захватить мир?” И результат у тебя перед глазами. | Во-вторых, волей-неволей задумываешься, обладаю ли я достаточным количеством знаний в той или иной области, чтобы еще и делиться своими мыслями на эту тему с другими людьми. | К тому же, число различных блогов растет день ото дня. Есть ли что-то такое, что будет отличать мой блог от сотни других? И все такое прочее. | . Но я сформулировала для себя несколько правил, которые эти проблемы решают, и на которые я и буду опираться в дальнейшем: . Данный блог я веду в первую очередь для себя. Я веду его, чтобы фиксировать свои идеи, чтобы напомнить мне будущей, какую большую работу я проделала, чтобы еще раз рассказать о том, что я узнала и утрамбовать свои знания в голове (кажется, это называется, закрепить полученные навыки). | Едва ли этот блог будет читать кто-то кроме меня. А если на него кто-нибудь чудом и наткнется, буду рада. Но если этому кому-то что-то не понравится - я никого тут не задерживаю. Этот блог не носит никаких коммерческих целей, поэтому нет цели завоевать признание миллионов. | Этот блог я веду в процессе обучения на курсе по машинному обучению. Ведение блога было рекомендацией авторов этого курса, поэтому, вероятно, на английском языке будет огромное число похожих блогов, но, думаю, на русском языке их число сильно меньше. | Что ж, вступление есть, теперь можно приступать к делу. . .",
            "url": "https://irinaprokofieva.github.io/my-ml-blog/markdown/2020/09/02/first-post.html",
            "relUrl": "/markdown/2020/09/02/first-post.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Искусственный интеллект и все-все-все",
            "content": "Очень часто люди, не имеющие дела с искуственным интеллектом лично, думают, что &quot;искусственный интеллект&quot;, &quot;машинное обучение&quot; и &quot;глубокое обучение&quot; - взаимо заменяемые словосочетания. Однако, это не совсем так. . Давайте посмотрим, какое определение искусственному интеллекту дает толковый словарь по искусственному интеллекту: . Интеллект Искусственный - научное направление, в рамках которого ставятся и решаются задачи аппаратного или программного моделирования тех видов человеческой деятельности, которые традиционно считаются интеллектуальными. . То есть это самый общий термин из всех трех, он, как можно догадаться, включает в себя все остальные, и обозначает целую область знаний. . Стоит заметить, что в обиходе под искусственным интеллектом обычно понимается умение машины выполнять какие-то действия наравне с человеком (или даже лучше). Здесь лучше подойдет второе определение: . Интеллект Искусственный - свойство интеллектуальных систем выполнять функции (творческие), которые традиционно считаются прерогативой человека. . #![](my_icons/IronNanny.png) . С искусственным интеллектом все вроде бы понятно. Этот термин можно почти безошибочно применять ко всему, что нам кажется достаточно умным для машины :) Перейдем к машинному обучению. Что это такое? Вот определение, которое я для себя вывела. . Машинное обучение - огромный подраздел искусственного интеллекта, характерной чертой которого является то, что алгоритм поиска решения некоторой задачи строится не на том, что мы пишем определенную последовательность команд, выполнение которых приводит к ответу, а на том, что машина сама находит закономерности в предоставленных ей данных и на основании этих закономерностей делает предсказания, каким может быть ответ. . Чтобы запомнить, что машинное обучение - только подраздел искуственного интеллекта, упомяну несколько других таких же подразделов: . Интеллектуальная робототехника | Экспертные системы | Машинное творчество и т.д. | . Конечно, зачастую эти подразделы переплетаются между собой. Главное, что отличает машинное обучение от остальных подразделов - самостоятельное получение машиной новых знаний. . Теперь глубокое обучение (deep learning). Глубокое обучение - один из видов машинного обучения. Это такой способ извлечения и обработки данных, который базируется на многослойных нейронных сетях. . Как это работает? Если в двух словах, то каждый слой сети состоит из нескольких нейронов. Каждый нейрон получает на вход данные от нейронов предыдущего уровня, обрабатывает полученную информацию, и передает обработанные данные нейронам из следующего слоя. . Это очень коротко и непонятно, но подробнее напишу об этом позже. Пока достаточно запомнить, что помимо глубокого обучения, есть еще такие виды машинного обучения, как: . Классические методы обучения (с учителем/без учителя) | Ансамблевые методы (Стеккинг, Беггинг, Бустинг) | Обучение с подкреплением | . Различных методов очень много и классифицировать их можно по-разному. Основное, что отличает глубокое обучение от остальных методов, это то, что обучение происходит на нейронных сетях с несколькими слоями. . Искусственный интеллект уже стал частью нашей повседневной жизни. Далее привожу примеры успешного использования ИИ. . Компьютерное зрение: . Распознавание номеров автомобилей | Распознавание лиц | Беспилотные автомобили | Поиск фотографий на телефоне по таким запросам, как &quot;закат&quot;, &quot;пикник&quot;, &quot;Новый год&quot; и т.д. | . В медицине: . Диагностирование рака | Нахождение различных патологий на рентгеновских снимках, МРТ и др. | . Обработка естественного языка: . Распознавание речи (&quot;Привет, Алиса&quot;, &quot;Ок, Google&quot; и т.п.) | Классификация документов по темам | Поддержание разоговора, ответы на вопросы (различные чат-боты) | Генерация текста | . А так же: . Рекомендательные системы: Расположение ссылок в поисковиках по релевантности, подбор фильмов, &quot;Вам также может понравиться...&quot;, контекстная реклама и вот это всё. . Игры: Искуственный интеллект уже обыграл человека в шахматы и Го, а помимо этого умеет проходить много других игр. . На видео: программист из Австралии научил ИИ играть в динозаврика из Chrome. Видео длинное, но посмотреть любопытно. И многое-многое другое... ИИ врывается во многие сферы жизни, и мы сами уже не замечаем, что встречаемся с ним ежедневно. . &#1050;&#1072;&#1082; &#1088;&#1072;&#1073;&#1086;&#1090;&#1072;&#1077;&#1090; &#1085;&#1077;&#1081;&#1088;&#1086;&#1085;&#1085;&#1072;&#1103; &#1089;&#1077;&#1090;&#1100;? . Представим ситуацию: нам нужно отправить большой архив фотографий бабушке, но при этом не хотим, чтобы она увидела, что кто-то из её любимых внуков завел дома удава. Руками перебирать все фотографии очень долго, поэтому мы хотим написать такую программу, которая сама могла бы определить, есть ли на фото удав или нет. Время выхода нейронной сети на сцену! . Мы хотим, чтобы общая концепция была такая: скармливаем фото программе, она что-то с ней делает, а нам только выдает результат: &quot;Все отлично, удава здесь нет&quot;. . На самом деле, программе не достаточно получить только фотографию, она еще хочет получить параметры - набор значений для её внутренних переменных, чтобы она точно знала, что делать с входными данными - искать удава или искать закат. То, что мы получим на выходе, напрямую зависит от этих самых параметров. . Откуда нам взять параметры? . С разными параметрами наша модель (модель - программа, поведение которой определяется параметрами) и предсказывает по-разному. Чтобы достичь максимума точности в таком непростом деле, как детектирование удава, было бы неплохо, если бы она сама умела сравнивать эффективность при разных параметрах и запоминать лучшие. А еще мы бы хотели, чтобы она сама умела подбирать параметры так, чтобы её точность повышалась. . Например, подаем мы ей на вход параметры (а1, а2, а3, ... ,аn) и набор фотографий с пометками о наличии на них удава. Модель берет фотографию и говорит: &quot;Здесь есть удав с вероятностью 60%&quot;. Потом смотрит в ответы и видит, что удав тут и на самом деле есть. Тогда она думает: &quot;Хм, параметр а10 сбил меня с мысли, а ведь параметры а11 и а25 мне говорили, что тут точно есть удав&quot;. Берет следующую фотографию. Точно так же делает предположение, сверяет с ответом, смотрит, какие параметры надо поменять. Когда она проделает это со всеми поданными на вход фотографиями, она определяет точность, с которой она сейчас умеет определять удава. Например, она находит удава на фото в 75% случаев. Теперь она предлагает поменять определенные параметры, например, а10 уменьшить, а а11 и а25 увеличить.Теперь с этими параметрами она снова берет все фотографии и проделывает все то же самое. И оп! С такими параметрами она находит удава в 83% случаев. Опять корректирует значения и так далее. Прелесть в том, что модель делает все это без нашей помощи и учится на своих ошибках. . При этом модели важно не просто то, что она сказала, что &quot;здесь есть удав&quot;, но и еще и то, с каким качеством она это сделала, насколько она уверена в своих результатах. И её цель - повысить именно это качество, стать увереннее в себе :), а не просто выдать какой-то ответ. . Когда нам нравится качество, с которым модель предсказывает результаты, мы сохраняем параметры, при которых оно достигается. Теперь наша модель уже обучена. В следующий раз нам не придется заново учить её опознавать удавов - мы загружаем ей свои фотографии (уже без ответов), а она, пользуясь параметрами, полученными в прошлый раз, сразу выдает результат. . . Внесем ясность: . Получается, обучение модели (fit, train) - подбор параметров таким образом, чтобы на помеченных данных точность предсказаний было максимальным. . Заметим, что модель состоит из некоторой функции (архитектуры) и параметров. . В качестве архитектуры модели тут как раз будут использоваться нейронные сети. . В качестве метода улучшения параметров - функции оптимизации - часто выступает SGD (стохастический градиентный спуск). . Результаты, которые выдает модель, называются предсказаниями (predictions). . Полное прохождение всех входных данных одного цикла называется эпохой (epoch). . Функция, с помощью которой измеряется качество, называется функцией потерь (loss). Функция потерь выбирается так, чтобы её понимала сама модель и на её основании делала вывод, какие параметры как поменять. Если же мы уже обучили нейросеть и хотим похвастаться другу, как она хороша, или если мы хотим более точно понимать, насколько хороша модель в конце каждой эпохи и какое качество она показывает на валидационной выборке (что это такое будет ниже), мы используем метрики. Метрика - понятная для человека функция, показывающая эффективность модели. . А правильные ответы, которые мы загружаем вместе с фотографиями при обучении модели и которые пытается отгадать модель, по-английски называются labels или targets, а в русском языке я их названий точно не знаю, может, таргеты или лейблы :) . . Итак, повторим еще раз: . В архитектуру загружаются входные данные (например, фотографии) и параметры. Архитектура делает предсказания. Затем они сверяются с таргетами и функция потерь выдает качество модели. С помощью функции оптимизации улучшаем параметры и снова загружаем их в архитектуру вместе в входными данными. Следующая эпоха. . Примечание: Думаю, очевидно, что после обучения модель сможет распознавать только такие шаблоны, которые встречала во входных данных. Если она училась только на фотографиях кошек, то она не сможет распознать кошку, нарисованную от руки. . Представим ситуацию: мы выдали модели большое число помеченных фотографий с удавами и без, обучение проходит идеально: точность 100%. Сохраняем эти параметры и на радостях загружаем фотографию, где мы обнимаемся с удавом. И вдруг результат: на этой фотографии удава нет. Как же так? Ведь модель обещала нам стопроцентную точность! Скорее всего, дело в том, что модель во время обучения просто запомнила все наши картинки и их таргеты, поэтому и выдавала к ним все время верные ответы, а саму суть (то, как выглядит удав) так и не поняла. Говорят, что модель ПЕРЕобучилась (overfitting). Чтобы этого избежать, обычно перед обучением входные помеченные данные делят на две группы - обучающая выборка (trainig set) и проверочная выборка (validation set). Обычно для проверки используют 20-30% всех данных. Процесс обучения проходит только на обучающей выборке, а эффективность модели в конце каждой эпохи оценивается на проверочной, потому что эти данные модель раньше точно не видела и точно не могла их выучить. Причем каждый раз эта валидационная выборка, однажды выбранная, должна оставаться одной и той же. В большинстве случаев данные, входящие в эту выборку выбираются из общего числа данных случайным образом, но в некоторых случаях оценка точности на случайной выборке не является адекватным критерием эффективности. Например, мы хотим каким-то образом научить модель предсказывать ближайший рост или падение акций компании N на основе имеющейся у нас истории поведения стоимости её акций за прошлые 10 лет. Если мы в валидационную выборку включим случайные даты из последних 10 лет, то модель может догадаться, сколько стоили акции на основе предыдущего и последующего дней. Однако в реальности у нас нет этого &quot;последующего&quot; дня: мы хотим угадать стоимость акций завтра, но не знаем, сколько они будут стоить послезавтра. Поэтому для проверки реальной эффективности, логичнее в валидиционную выборку включить, например, последние 2 месяца из имеющейся у нас истории и посмотреть, насколько предсказания модели соответствуют действительности. То есть если наши предсказания будут каким-то образом упорядочены по оси времени, то к формированию проверочной выборки надо относиться аккуратнее! . Примечание: Несмотря на то, что мы заблаговременно разделили наши данные на обучающие и проверочные, мы все равно можем столкнуться с проблемой запоминания данных. Но так мы хотя бы сможем установить этот факт уже в процессе обучения, а не после: точность на обучающей выборке будет расти, в то время как точность на валидационной выборке будет ухудшаться. . Обычно присутствует еще тестовая выборка (test set), для самой финальной проверки. Её модель не видит ни разу за время обучения, и ею проверяют точность на самой последней стадии работы. Например, на соревнованиях эту выборку не видят участники, она доступна только для проверки сданных работ(моделей). Или если вам делают модель на заказ, то часть размеченных данных заказчик может оставить себе, не показывая разработчику, и уже при приемке готовой работы проверить точность модели на этих данных. . Конечно, модель можно полностью написать самому. Но если вы не тот самый человек, который готов потратить огромное количество времени на написание собственной модели, не будучи уверенным, что она даст сильный выигрыш по времени, то будет гораздо удобнее взять уже готовые модели и дообучать их на своем конкретном случае. Такие модели называются предобученными, так как они уже обучились на каких-то больших наборах данных (dataset). Таких готовых моделей существует очень много. Например, для классификации изображений существуют такие готовые модели, как ResNet или VGG. Они обучены уже на огромном датасете ImageNet и уже умеют находить на изображении множество разных шаблонов. . ImageNet - один из самых известных бесплатных больших дасатетов. Он насчитывает наборы изображений для более, чем 100000 классов. Для каждого класса предоставляется в среднем 1000 картинок. Пометки о том, к какому классу относится изображение, сделаны людьми, и качество данных контролируется. . Для задач компьютерного зрения используются сверточные нейронные сети, сокращенно - CNN (Convolutional neural network). Как несложно догадаться, они используют операцию свертки. О том, как именно они работают будет в одной из следующих статей. . При работе с изображениями перед обучением данные обычно обрабатывают различными способами, чтобы получить еще больше данных. Например, увеличивают яркость, вращают, растягивают, добавляют шум и т.д. Английский термин для этого - data augmentation. Но в любом случае перед загрузкой в модель все файлы приводят к одинаковому размеру. Обычно можно увидеть приведение к размеру 224x224, но число 224 не несет в себе особой смысловой нагрузки. Так сложилось исторически и не более чем формальность - вы можете выбрать другой размер. Чем больше размер - тем лучших результатов можно достичь, правда, ценой будет более долгий процесс обучения и бОльшие требуемые мощности. . На сегодня, пожалуй, хватит. В этой статье я ввела множество основных терминов, но не углублялась в теорию. Если у вас есть вопросы или замечания, пишите в комментариях, а я пошла готовить следующую статью. До новых встреч! . Photo by Jan Tinneberg on Unsplash .",
            "url": "https://irinaprokofieva.github.io/my-ml-blog/2020/09/02/IA-and-everything.html",
            "relUrl": "/2020/09/02/IA-and-everything.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://irinaprokofieva.github.io/my-ml-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://irinaprokofieva.github.io/my-ml-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://irinaprokofieva.github.io/my-ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://irinaprokofieva.github.io/my-ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}